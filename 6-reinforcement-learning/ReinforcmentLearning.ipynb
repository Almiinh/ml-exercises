{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.24.3', '2.0.2', '3.7.1')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib\n",
    "\n",
    "import numpy             as np\n",
    "import pandas            as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.__version__, pd.__version__, matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Type, Dict, Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I.Implementation of the Q-learning algorithm\n",
    "\n",
    "- In this part, we will implement a reinforcement learning algorithm.\n",
    "- Two classes will be implemented: \n",
    "    1. The agent \n",
    "    2. The environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### I.1. Agent\n",
    "\n",
    "Here, we start by implementing the agent's functions.\n",
    "\n",
    "#### I.1.1. Creation of the Q table\n",
    "\n",
    "Given $n$ states and $m$ actions, we create a matrix $Q[n, m]$ initialized with $0$'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the function to create the table Q\n",
    "\n",
    "def create_Q(nb_states: int, nb_actions: int) -> np.ndarray:\n",
    "    # hint : use numpy function to generate the table\n",
    "    # Return a new array of given shape, filled with zeros. \n",
    "    \n",
    "    ### CODE 01 ###\n",
    "    ### BEGIN : Write your code here\n",
    "    \n",
    "    Q_table = np.zeros((nb_states,nb_actions))\n",
    "    \n",
    "    ### END\n",
    "    \n",
    "    return Q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# array([[0., 0., 0.],\n",
    "#        [0., 0., 0.],\n",
    "#        [0., 0., 0.],\n",
    "#        [0., 0., 0.],\n",
    "#        [0., 0., 0.]])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "Q5_3 = create_Q(5, 3)\n",
    "\n",
    "Q5_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.2. Exploration & Exploitation of the Q table\n",
    "- In both functions, we choose an integer between $0$ and $m$ (number of actions).\n",
    "- In the exploration function, we choose this number randomly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the exploration function\n",
    "\n",
    "def exploration(Q: np.ndarray) -> int:\n",
    "    # hint :  Use np.random.randint() \n",
    "    \n",
    "    ### CODE 02 ###\n",
    "    ### BEGIN : Write your code here    \n",
    "    \n",
    "    nb_actions = Q.shape[1]\n",
    "    action_to_explore = np.random.randint(0,nb_actions)\n",
    "    \n",
    "    ### END    \n",
    "    \n",
    "    return action_to_explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# a random number in {0, 1, 2}\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "exploration(Q5_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the exploitation, the action with the maximum value is chosen from those of the current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the exploitation function\n",
    "\n",
    "def exploitation(Q: np.ndarray, state: int) -> int:\n",
    "    \"\"\"\n",
    "    Returns the indice of the best action for the selected state, the action with the maximum values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Q : array_like, Input array, The Q table/matrix.\n",
    "    state : indice of the stat.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    index_best_action : int\n",
    "    \n",
    "    \"\"\"\n",
    "    # Hint : Remmeber that Rows and Columns represent respectively the possible \"States\" and \"Actions\" of the agent.\n",
    "    # Return the indice not the maximum value\n",
    "\n",
    "    ### CODE 03 ###\n",
    "    ### BEGIN : Write your code here  \n",
    "    \n",
    "    index_best_action = np.argmax(Q[state,:])\n",
    "    \n",
    "    ### END\n",
    "    return index_best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 0, 1, 2, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# (2, 0, 1, 2, 1)\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "Q_t = np.array([\n",
    "    [0.1, 0.2, 0.3],\n",
    "    [1.0, 0.5, 0.7],\n",
    "    [0.5, 1.0, 0.8],\n",
    "    [0.2, 0.8, 0.9],\n",
    "    [0.2, 1.0, 0.3]\n",
    "])\n",
    "\n",
    "exploitation(Q_t, 0), exploitation(Q_t, 1), exploitation(Q_t, 2), exploitation(Q_t, 3), exploitation(Q_t, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(Q: np.ndarray, state: int, epsilon: float=0.2) -> int:\n",
    "    if np.random.random() < epsilon:\n",
    "        return exploration(Q)\n",
    "    else:\n",
    "        return exploitation(Q, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result: \n",
    "# Either 2 or a random number in {0, 1, 2}\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "choose_action(Q_t, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.3. Update of the table Q\n",
    "\n",
    "This equation is how our Q-values are updated over time :\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) = Q(s_t, a_t) + \\alpha * (r + \\gamma * \\max_a Q(s_{t+1}, a) - Q(s_t, a_t))\n",
    "$$\n",
    "\n",
    "With :\n",
    "- $Q(s_t, a_t)$ (```Q```): the entry $[s_t,a_t]$ of the $Q$ table.\n",
    "- $\\alpha $ : Is the learning rate (```alpha```).\n",
    "- $ s_t $ : The current state (```state```).\n",
    "- $ a_t $ : the choosed action to take `(```action```).\n",
    "- $ r $ : Immediate reward (```r```). \n",
    "- $ \\gamma $ : Discount-factor (```gamma```).\n",
    "- $ Q(s_{t+1}, a) $ : the next state $s_{t+1}$ (```next_state```)  in the Q-table with all possible actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement the update function of the table Q\n",
    "\n",
    "def update_Q(Q: np.ndarray, state: int, next_state: int, action: int, alpha: float, r: float, gamma: float) -> np.ndarray:\n",
    "    new_Q = Q.copy()\n",
    "    \n",
    "    ### CODE 04 ###\n",
    "    ### BEGIN : Write your code here  \n",
    "    \n",
    "    new_Q[state,action] = Q[state,action] + alpha*(r + gamma*Q[next_state, exploitation(Q, next_state)] - Q[state, action])\n",
    "    \n",
    "    ### END\n",
    "    \n",
    "    return new_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.1 , 0.2 , 0.3 ],\n",
       "       [1.  , 0.5 , 1.58],\n",
       "       [0.5 , 1.  , 0.8 ],\n",
       "       [0.2 , 0.8 , 0.9 ],\n",
       "       [0.2 , 1.  , 0.3 ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result : \n",
    "# array([[0.1 , 0.2 , 0.3 ],\n",
    "#        [1.  , 0.5 , 1.58],\n",
    "#        [0.5 , 1.  , 0.8 ],\n",
    "#        [0.2 , 0.8 , 0.9 ],\n",
    "#        [0.2 , 1.  , 0.3 ]])\n",
    "#---------------------------------------------------------------------\n",
    "Q_t = np.array([\n",
    "    [0.1, 0.2, 0.3],\n",
    "    [1.0, 0.5, 0.7],\n",
    "    [0.5, 1.0, 0.8],\n",
    "    [0.2, 0.8, 0.9],\n",
    "    [0.2, 1.0, 0.3]\n",
    "])\n",
    "\n",
    "update_Q(Q_t, state=1, next_state=2, action=2, alpha=0.2, r=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.1.4. La classe Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, nb_states: int, nb_actions: int, alpha: float, epsilon=0.2):\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.Q = create_Q(nb_states, nb_actions)\n",
    "    \n",
    "    def set_state(self, state: int):\n",
    "        self.state = state\n",
    "        self.action = 0\n",
    "        \n",
    "    def choose_action(self):\n",
    "        self.action = choose_action(self.Q, self.state, self.epsilon)\n",
    "        return self.action\n",
    "    \n",
    "    def apply(self, next_state: int, action: int, r: float, gamma: float):\n",
    "        self.Q = update_Q(self.Q, self.state, next_state, self.action, self.alpha, r, gamma)\n",
    "        self.state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.  ,  4.  ,  0.  ],\n",
       "       [-0.36, -0.2 , -2.  ],\n",
       "       [ 0.  ,  0.  ,  0.  ],\n",
       "       [-0.2 ,  0.4 ,  0.  ],\n",
       "       [ 2.  ,  0.  ,  0.  ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result : \n",
    "# array([[-2.  ,  4.  ,  0.  ],\n",
    "#        [-0.36, -0.2 , -2.  ],\n",
    "#        [ 0.  ,  0.  ,  0.  ],\n",
    "#        [-0.2 ,  0.4 ,  0.  ],\n",
    "#        [ 2.  ,  0.  ,  0.  ]])\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "next_stats_rs = [(0, -1), \n",
    "                 (1, -10), \n",
    "                 (3, -1), \n",
    "                 (1, 2), \n",
    "                 (4, -1), \n",
    "                 (1, 10), \n",
    "                 (1, -10), \n",
    "                 (0, -1), \n",
    "                 (0, 20)]\n",
    "\n",
    "# exploitation: to make it deterministic\n",
    "agent = Agent(nb_states=5, nb_actions=3, alpha=0.2, epsilon=0.) \n",
    "\n",
    "# initial state = 3\n",
    "agent.set_state(state=3)\n",
    "\n",
    "for next_state, r in next_stats_rs:\n",
    "    action = agent.choose_action()\n",
    "    # Environment FeedBack (next_state, r)\n",
    "    agent.apply(next_state, action, r, gamma=0.5)\n",
    "    \n",
    "agent.Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I.2. Environment\n",
    "\n",
    "- Here we will implement the cab and passenger problem: (https://arxiv.org/pdf/cs/9905014.pdf).\n",
    "- Our environment is a space divided into $nb_l$ rows and $nb_c$ columns to indicate the position of the cab.\n",
    "- It also contains a number of stops $nb_a$.\n",
    "- The position of the cab is encoded using the row and column number (the coordinates).\n",
    "- The destination is the stop number ($0 \\leq dst < nb_a$).\n",
    "- The position of the passenger is represented by the stop number ```psg``` ($0 \\leq psg < nb_a$) :\n",
    "    - We set $psg = nb_a$ to indicate that the passenger is inside the cab.\n",
    "\n",
    "#### I.2.1. Encoding and decoding of the states\n",
    "\n",
    "Here we have two functions: \n",
    "- One that encodes the state based on :\n",
    "    1. ```pos```$(x,y)$ : The position of the cab.\n",
    "    2. The number of the startup stop.\n",
    "    3. ```dst``` The number of the destination stop.\n",
    "    4. The number of columns ```nb_c```, rows ```nb_l``` and stops ```nb_a```.\n",
    "\n",
    "with the following equation:\n",
    "    \n",
    "$$ (x * nb_c + y) * (nb_a + 1) * nb_a + (psg * nb_a + dst) $$\n",
    "\n",
    "- The other function decodes a state of position of the cab: \n",
    "    1. Line \n",
    "    2. Column \n",
    "    3. Passenger stop \n",
    "    4. The destination stop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder_state(pos: Tuple[int, int], psg: int, dst: int, nb_l: int, nb_c: int, nb_a: int) -> int:\n",
    "    \n",
    "    x = pos[0]\n",
    "    y = pos[1]\n",
    "    \n",
    "    ### CODE 05 ###\n",
    "    ### BEGIN : Write your code here  \n",
    "    \n",
    "    encoding = (x*nb_c + y)*(nb_a + 1)*nb_a + (psg*nb_a + dst)\n",
    "    \n",
    "    ### END\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result : \n",
    "# 153\n",
    "#---------------------------------------------------------------------\n",
    "encoder_state((1, 2), 3, 1, 5, 5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_state(state: int, nb_l:int, nb_c: int, nb_a: int) -> Tuple[int, int, int, int]:\n",
    "    # max number of passenger positions * stop per box\n",
    "    nb_pa = (nb_a + 1) * nb_a \n",
    "    \n",
    "    # passenger position * stop position\n",
    "    pa  = state % nb_pa \n",
    "    dst = pa % nb_a\n",
    "    psg = pa // nb_a\n",
    "    \n",
    "    # Row * Col\n",
    "    lc = state // nb_pa \n",
    "    l  = lc // nb_c\n",
    "    c  = lc % nb_c\n",
    "    \n",
    "    return l, c, psg, dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result : \n",
    "# (1, 2, 3, 1)\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "decoder_state(153, 5, 5, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.2. Calculer la r√©compense\n",
    "\n",
    "The reward function has as input: \n",
    "\n",
    "- ```state``` : the current state of the agent.\n",
    "- ```action``` : number of the action chosen by the agent \n",
    "- ```nb_l``` , ```nb_c``` : number of rows and columns in the environment\n",
    "- ```stops``` : a list of stop positions:\n",
    "    - The position is encoded as a tuple (x, y). \n",
    "    - P.S. The tuples are hashable; therefore we can check their existence in the list by using the ```in``` operator.\n",
    "- ```bar``` : a dictionary {pos: list integers} : If a position exists, we will have a list of disallowed  actions (positioning actions).\n",
    "\n",
    "The function must return: \n",
    "    A. the reward\n",
    "    B. the next state \n",
    "    C. And a boolean that indicates the end of the process.\n",
    "\n",
    "**A. The reward:**\n",
    "\n",
    "- For each action performed, a reward of -1 is given, to force the agent to go ahead and make the shortest possible path.\n",
    "- If the agent attempts to drop off or pick up a passenger illegally, a reward of -10 is awarded in addition.\n",
    "    - The \"drop off\" action: is considered illegal (if the passenger is not in the cab) **OR** (if the drop off location is not the destination stop).\n",
    "    - The action \"pick up\": is considered illegal (if the passenger is not in the current position) **OR** (he is already in the car).\n",
    "- If the agent drops off the passenger at the destination stop, it will get an additional +20 reward.\n",
    "\n",
    "**B. The next state:**\n",
    "The next state is the current state except in the following cases:\n",
    "- If a passenger is picked up successfully, the passenger's index will be ```nb_a``` (in the car). \n",
    "- If a passenger is successfully dropped off, the passenger index will be ```dst```. The end will be ```True```.\n",
    "- In the case of a positional action ({0, 1, 2, 3}), if the position is not in the list of barrier ```bar``` or it exists but the action does not exist in the list of forbidden actions, the new position and the new state are calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on the action and the position, return the new position.\n",
    "# This function does not take into account the constraints\n",
    "\n",
    "def move(pos: Tuple[int, int], action: int) -> Tuple[int, int]:  \n",
    "    # action = 0 (Move to the left)\n",
    "    # action = 1 (Move to the right) \n",
    "    # action = 2 (Move forwards)\n",
    "    # action = 3 (Move backwards)\n",
    "    # action = 4 (Pick up a passenger)\n",
    "    # action = 5 (Drop off a passenger)\n",
    "    \n",
    "    if action == 0: \n",
    "        return pos[0], pos[1]  - 1\n",
    "    if action == 1: \n",
    "        return pos[0], pos[1]  + 1\n",
    "    if action == 2: \n",
    "        return pos[0] + 1, pos[1]\n",
    "    if action == 3: \n",
    "        return pos[0] - 1, pos[1]\n",
    "    \n",
    "    # in case the \"action > 3\"\n",
    "    \n",
    "    return pos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 2), (1, 2), (1, 1))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result : \n",
    "# ((2, 2), (1, 2), (1, 1))\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "move((1, 2), 2), move((1, 2), 4), move((1, 2), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Complete the function that calculates the reward\n",
    "\n",
    "def calculate_reward(state: int, action: int, \n",
    "                        nb_l:int, nb_c: int, \n",
    "                        stops: List[Tuple[int, int]],\n",
    "                        bar: Dict[Tuple[int, int], Set[int]]) -> Tuple[float, int, bool]:\n",
    "    \n",
    "    # number of stops\n",
    "    nb_a = len(stops)\n",
    "    \n",
    "    # Decode State \n",
    "    l, c, psg, dst = decoder_state(state, nb_l, nb_c, nb_a)\n",
    "    # Taxi Position \n",
    "    pos = (l, c)\n",
    "    # Passenger destination position\n",
    "    destination_pos = stops[dst] \n",
    "    \n",
    "    # The reward:\n",
    "    ##  Always apply this reward\n",
    "    reward = -1\n",
    "    next_state = state\n",
    "    fin = False\n",
    "    \n",
    "    # Hint : translate the reward and changing stat rules into if else statements \n",
    "    \n",
    "    ### CODE 06 ###\n",
    "    ### BEGIN : Write your code here   \n",
    "    \n",
    "    \n",
    "    # if action is in {0,1,2,3} : positional action\n",
    "    if action in [0,1,2,3] : \n",
    "        if pos not in bar : # If the taxi has not been barred\n",
    "            pos = move(pos, action)  \n",
    "            next_state = encoder_state(pos, psg, dst, nb_l, nb_c, nb_a)\n",
    "        elif ( pos in bar and action not in bar[pos] ) :\n",
    "            pos = move(pos, action)  \n",
    "            next_state = encoder_state(pos, psg, dst, nb_l, nb_c, nb_a)\n",
    "            \n",
    "            \n",
    "    # action = 4 (Pick up a passenger)\n",
    "    if (action == 4) : \n",
    "        \n",
    "        # Illegal pick up\n",
    "        # if the passenger is not in the current position **OR** he is already in the car. \n",
    "        # a -10 reward is given in addition.\n",
    "        if psg < nb_a :     # passenger at a stop\n",
    "            if pos != stops[psg] : # and taxi not at a stop\n",
    "                reward -= 10\n",
    "            else:\n",
    "                psg = nb_a\n",
    "                next_state = encoder_state(pos, psg, dst, nb_l, nb_c, nb_a)         \n",
    "        elif psg == nb_a :  # passenger in the car\n",
    "            reward -= 10\n",
    "        \n",
    "    # action = 5 (Drop off a passenger)\n",
    "    if (action == 5) : \n",
    "        \n",
    "        # Illegal drop off \n",
    "        # if the passenger is not in the cab **OR** if the drop-off location is not the destination stop.\n",
    "        # a -10 reward is given in addition.\n",
    "        if psg < nb_a or pos != destination_pos :\n",
    "            reward += -10\n",
    "            \n",
    "        # If the agent successfully drops off the passenger at the destination stop\n",
    "        # he will get a +20 reward\n",
    "        # the passenger's index will be dst. \n",
    "        # The end will be True.\n",
    "        elif pos == destination_pos :\n",
    "            reward += 20\n",
    "            psg = dst\n",
    "            next_state = encoder_state(pos,psg,dst,nb_l,nb_c,nb_a)\n",
    "            fin = True\n",
    "            \n",
    "    ### End       \n",
    "\n",
    "\n",
    "    return reward, next_state, fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_l, nb_c = 5, 5\n",
    "stops = [(0,0), (0,4), (4,0), (4,3)]\n",
    "nb_a = len(stops)\n",
    "barriers = {\n",
    "    (0, 1): set([1]), # barrier on the right\n",
    "    (0, 2): set([0]), # barrier on the left\n",
    "    \n",
    "    (3, 0): set([1]), # barrier on the right\n",
    "    (3, 1): set([0]), # barrier on the left\n",
    "    \n",
    "    (4, 0): set([1]), # barrier on the right\n",
    "    (4, 1): set([0]), # barrier on the left\n",
    "    \n",
    "    (3, 2): set([1]), # barrier on the right\n",
    "    (3, 3): set([0]), # barrier on the left\n",
    "    \n",
    "    (4, 2): set([1]), # barrier on the right\n",
    "    (4, 3): set([0]), # barrier on the left\n",
    "}\n",
    "\n",
    "tests = [\n",
    "    # (state, action)\n",
    "    # action = 4 (Pick up a passenger)\n",
    "    (encoder_state(pos=(0, 2), psg=4, dst=0, nb_l=nb_l, nb_c=nb_c, nb_a=nb_a), 4), ## pos=(0,2); psg=in the cab; dst=stop0(0, 0)\n",
    "    (encoder_state(pos=(0, 2), psg=1, dst=0, nb_l=nb_l, nb_c=nb_c, nb_a=nb_a), 4), ## pos=(0,2); psg=stop1; dst=stop0(0, 0)\n",
    "    (encoder_state(pos=(0, 4), psg=1, dst=0, nb_l=nb_l, nb_c=nb_c, nb_a=nb_a), 4), ## pos=stop1(0,4); psg=stop1(0,4); dst=stop0(0, 0)\n",
    "    \n",
    "    # action = 5 (Drop off a passenger)\n",
    "    (encoder_state((0, 0), 4, 0, nb_l, nb_c, nb_a), 5), # pos=stop0; psg=in the cab; dst=stop0(0, 0)\n",
    "    (encoder_state((0, 0), 1, 0, nb_l, nb_c, nb_a), 5), # pos=stop0; psg=stop1; dst=stop0(0, 0)\n",
    "    (encoder_state((0, 2), 4, 0, nb_l, nb_c, nb_a), 5), # pos=(0, 2); psg=in the cab e; dst=stop0(0, 0)\n",
    "    (encoder_state((0, 2), 1, 0, nb_l, nb_c, nb_a), 5), # pos=(0, 2); psg=stop1; dst=stop0(0, 0)\n",
    "    \n",
    "    # action = 0 (Move to the left)\n",
    "    (encoder_state((0, 2), 1, 0, nb_l, nb_c, nb_a), 0), # there is a barrier on the left\n",
    "    (encoder_state((3, 0), 1, 0, nb_l, nb_c, nb_a), 0), # there is a barrier on the right\n",
    "    (encoder_state((2, 2), 1, 0, nb_l, nb_c, nb_a), 0), # there is no barrier\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-11, 56, False),\n",
       " (-11, 44, False),\n",
       " (-1, 96, False),\n",
       " (19, 0, True),\n",
       " (-11, 4, False),\n",
       " (-11, 56, False),\n",
       " (-11, 44, False),\n",
       " (-1, 44, False),\n",
       " (-1, 284, False),\n",
       " (-1, 224, False)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=====================================================================\n",
    "# UNIT TEST\n",
    "#=====================================================================\n",
    "# Result : \n",
    "# [(-11, 56, False),\n",
    "#  (-11, 44, False),\n",
    "#  (-1, 96, False),                 \n",
    "#  (19, 0, True),\n",
    "#  (-11, 4, False),\n",
    "#  (-11, 56, False),\n",
    "#  (-11, 44, False),\n",
    "#  (-1, 44, False),\n",
    "#  (-1, 284, False),\n",
    "#  (-1, 224, False)]\n",
    "#---------------------------------------------------------------------\n",
    "results = []\n",
    "\n",
    "for state, action in tests:\n",
    "    results.append(calculate_reward(state, action, nb_l, nb_c, stops, barriers))\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I.2.3. The Environment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "END\n"
     ]
    }
   ],
   "source": [
    "import time, sys\n",
    "from IPython.display import HTML, display, clear_output\n",
    "\n",
    "class TaxiEnv():\n",
    "    def __init__(self, nb_l:int, nb_c: int, \n",
    "                 stops: List[Tuple[int, int]], \n",
    "                 bar: Dict[Tuple[int, int], Set[int]], gamma: float = 0.5):\n",
    "        self.actions = ['Left', 'right', 'moving forward', 'backwards', 'Pick-up', 'drop-off']\n",
    "        self.stops = stops\n",
    "        self.nb_l = nb_l\n",
    "        self.nb_c = nb_c\n",
    "        self.nb_states = nb_l * nb_c * (len(stops) + 1) * len(stops)\n",
    "        self.bar = bar\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        for i in range(nb_l):\n",
    "            pos = (i, 0)\n",
    "            if pos not in bar:\n",
    "                bar[pos] = set()\n",
    "                \n",
    "            # we can't go left\n",
    "            bar[pos].add(0) \n",
    "            \n",
    "            pos = (i, nb_c-1)\n",
    "            if pos not in bar:\n",
    "                bar[pos] = set()\n",
    "                \n",
    "            # we can't go right\n",
    "            bar[pos].add(1) \n",
    "            \n",
    "        for j in range(nb_c):\n",
    "            pos = (0, j)\n",
    "            \n",
    "            if pos not in bar:\n",
    "                bar[pos] = set()\n",
    "            # we can't go forward\n",
    "            bar[pos].add(3) \n",
    "            \n",
    "            pos = (nb_l-1, j)\n",
    "            if pos not in bar:\n",
    "                bar[pos] = set()\n",
    "            # we can't go backwards\n",
    "            bar[pos].add(2) \n",
    "            \n",
    "        \n",
    "    def add_agent(self, alpha: float, epsilon=0.2):\n",
    "        self.agent = Agent(self.nb_states, len(self.actions), alpha, epsilon=epsilon)\n",
    "    \n",
    "    def encoder_state(self, pos: Tuple[int, int], psg: int, dst: int):\n",
    "        return encoder_state(pos, psg, dst, self.nb_l, self.nb_c, len(self.stops))\n",
    "    \n",
    "    def decoder_state(self, state: int) -> Tuple[int, int, int]:\n",
    "        return decoder_state(state, self.nb_l, self.nb_c, len(self.stops))\n",
    "    \n",
    "    def initialize(self, pos: Tuple[int, int], psg: int, dst: int):\n",
    "        state = self.encoder_state(pos, psg, dst)\n",
    "        self.agent.set_state(state)\n",
    "    \n",
    "    def transporter(self, plot=False):\n",
    "        nb_l = self.nb_l\n",
    "        nb_c = self.nb_c\n",
    "        stops = self.stops\n",
    "        bar = self.bar\n",
    "        nb_a = len(stops)\n",
    "        actions = self.actions\n",
    "        state = self.agent.state\n",
    "        \n",
    "        etapes = []\n",
    "        fin = False\n",
    "        rt = 0\n",
    "        \n",
    "        while not fin:\n",
    "            action = self.agent.choose_action()\n",
    "            r, next_state, fin = calculate_reward(state, action, nb_l, nb_c, stops, bar)\n",
    "            etapes.append((self.decoder_state(state), actions[action], r, fin))\n",
    "            self.agent.apply(next_state, action, r, self.gamma)\n",
    "            if plot:\n",
    "                rt += r\n",
    "                html = self.draw()\n",
    "                html += '<div class=\"cont\">'\n",
    "                html += f'<p>Step: {len(etapes)}</p>'\n",
    "                html += f'<p>State: {state}</p>'\n",
    "                html += f'<p>Action: {actions[action]}</p>'\n",
    "                html += f'<p>Reward: {r}</p>'\n",
    "                html += f'<p>Sum of Rewards: {rt}</p>'\n",
    "                html += '</div>'\n",
    "                time.sleep(0.5)\n",
    "                clear_output(wait=True)\n",
    "                display(HTML(html))\n",
    "                sys.stdout.flush()\n",
    "            state = next_state\n",
    " \n",
    "        return etapes\n",
    "    \n",
    "    def draw(self):\n",
    "        bordures = ['l', 'r', 'b', 't']\n",
    "        \n",
    "        nb_a = len(self.stops)\n",
    "        \n",
    "        if hasattr(self.agent, 'state'):\n",
    "            l, c, psg, dst = decoder_state(self.agent.state, self.nb_l, self.nb_c, nb_a)\n",
    "        else:\n",
    "            l, c, psg, dst = None, None, None, None\n",
    "        \n",
    "        html = \"\"\"<style>\n",
    "                div.cont {display:inline-block; margin:5px; vertical-align: top;}\n",
    "                table#t, table#t td, table#t tr {border: 1px dotted black; \n",
    "                                                background: white; padding: 1px;}\n",
    "                \n",
    "                table#t td {width: 1cm; height:1cm; text-align: center;}\n",
    "                table#t tr td.l {border-left: 2px solid red ;}\n",
    "                table#t tr td.r {border-right: 2px solid red ;}\n",
    "                table#t tr td.b {border-bottom: 2px solid red ;}\n",
    "                table#t tr td.t {border-top: 2px solid red ;}\n",
    "                table#t tr td.stop {background: yellow;}\n",
    "                </style>\n",
    "                <div class=\"cont\">\n",
    "                <table id=\"t\">\n",
    "                \"\"\"\n",
    "        for i in range(self.nb_l):\n",
    "            html += \"<tr>\"\n",
    "            for j in range(self.nb_c):\n",
    "                cls = None\n",
    "                html += \"<td \"\n",
    "                if (i, j) in self.bar:\n",
    "                    cls = 'class=\"'\n",
    "                    bl = self.bar[(i, j)]\n",
    "                    for b in bl:\n",
    "                        cls += bordures[b] + ' '\n",
    "                if (i, j) in self.stops:\n",
    "                    if not cls:\n",
    "                        cls = 'class=\"'\n",
    "                    cls += 'stop'\n",
    "                if cls:\n",
    "                    cls += '\"'\n",
    "                    html += cls\n",
    "                html += '>'\n",
    "                cont = ''\n",
    "                if dst != None and self.stops[dst] == (i, j):\n",
    "                    cont = 'üè≤'\n",
    "                if psg != None and psg != nb_a and self.stops[psg] == (i, j):\n",
    "                    cont += 'üëΩ'\n",
    "                if (l, c) == (i, j):\n",
    "                    if psg != None and psg != nb_a:\n",
    "                        cont += 'üöñ'\n",
    "                    else:\n",
    "                        cont += 'üöç'\n",
    "                if not cont:\n",
    "                    cont = ':'\n",
    "                html += cont + '</td>'\n",
    "            html += '</tr>'\n",
    "            \n",
    "        html += '</table></div>'\n",
    "        \n",
    "        return html\n",
    "        \n",
    "print('END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "                div.cont {display:inline-block; margin:5px; vertical-align: top;}\n",
       "                table#t, table#t td, table#t tr {border: 1px dotted black; \n",
       "                                                background: white; padding: 1px;}\n",
       "                \n",
       "                table#t td {width: 1cm; height:1cm; text-align: center;}\n",
       "                table#t tr td.l {border-left: 2px solid red ;}\n",
       "                table#t tr td.r {border-right: 2px solid red ;}\n",
       "                table#t tr td.b {border-bottom: 2px solid red ;}\n",
       "                table#t tr td.t {border-top: 2px solid red ;}\n",
       "                table#t tr td.stop {background: yellow;}\n",
       "                </style>\n",
       "                <div class=\"cont\">\n",
       "                <table id=\"t\">\n",
       "                <tr><td class=\"l t stop\">üè≤üëΩüöñ</td><td class=\"r t \">:</td><td class=\"l t \">:</td><td class=\"t \">:</td><td class=\"r t stop\">:</td></tr><tr><td class=\"l \">:</td><td >:</td><td >:</td><td >:</td><td class=\"r \">:</td></tr><tr><td class=\"l \">:</td><td >:</td><td >:</td><td >:</td><td class=\"r \">:</td></tr><tr><td class=\"l r \">:</td><td class=\"l \">:</td><td class=\"r \">:</td><td class=\"l \">:</td><td class=\"r \">:</td></tr><tr><td class=\"l r b stop\">:</td><td class=\"l b \">:</td><td class=\"r b \">:</td><td class=\"l b stop\">:</td><td class=\"r b \">:</td></tr></table></div><div class=\"cont\"><p>Step: 416</p><p>State: 16</p><p>Action: drop-off</p><p>Reward: 19</p><p>Sum of Rewards: -1166</p></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stops = [(0,0), (0,4), (4,0), (4,3)]\n",
    "barriers = {\n",
    "    (0, 1): set([1]), # barrier on the right\n",
    "    (0, 2): set([0]), # barrier on the left\n",
    "    (3, 0): set([1]), # barrier on the right\n",
    "    (4, 0): set([1]), # barrier on the right\n",
    "    (3, 1): set([0]), # barrier on the left\n",
    "    (4, 1): set([0]), # barrier on the left\n",
    "    (3, 2): set([1]), # barrier on the right\n",
    "    (4, 2): set([1]), # barrier on the right\n",
    "    (3, 3): set([0]), # barrier on the left\n",
    "    (4, 3): set([0]), # barrier on the left\n",
    "}\n",
    "\n",
    "taxi = TaxiEnv(nb_l=5,nb_c=5, stops=stops, bar=barriers)\n",
    "\n",
    "        \n",
    "taxi.add_agent(alpha=0.1, epsilon=0.1)\n",
    "taxi.initialize(pos=(3, 1), psg=2, dst=0)\n",
    "\n",
    "\n",
    "html = taxi.draw()\n",
    "display(HTML(html))\n",
    "hist = taxi.transporter(plot=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "                div.cont {display:inline-block; margin:5px; vertical-align: top;}\n",
       "                table#t, table#t td, table#t tr {border: 1px dotted black; \n",
       "                                                background: white; padding: 1px;}\n",
       "                \n",
       "                table#t td {width: 1cm; height:1cm; text-align: center;}\n",
       "                table#t tr td.l {border-left: 2px solid red ;}\n",
       "                table#t tr td.r {border-right: 2px solid red ;}\n",
       "                table#t tr td.b {border-bottom: 2px solid red ;}\n",
       "                table#t tr td.t {border-top: 2px solid red ;}\n",
       "                table#t tr td.stop {background: yellow;}\n",
       "                </style>\n",
       "                <div class=\"cont\">\n",
       "                <table id=\"t\">\n",
       "                <tr><td class=\"l t stop\">üè≤üëΩüöñ</td><td class=\"r t \">:</td><td class=\"l t \">:</td><td class=\"t \">:</td><td class=\"r t stop\">:</td></tr><tr><td class=\"l \">:</td><td >:</td><td >:</td><td >:</td><td class=\"r \">:</td></tr><tr><td class=\"l \">:</td><td >:</td><td >:</td><td >:</td><td class=\"r \">:</td></tr><tr><td class=\"l r \">:</td><td class=\"l \">:</td><td class=\"r \">:</td><td class=\"l \">:</td><td class=\"r \">:</td></tr><tr><td class=\"l r b stop\">:</td><td class=\"l b \">:</td><td class=\"r b \">:</td><td class=\"l b stop\">:</td><td class=\"r b \">:</td></tr></table></div><div class=\"cont\"><p>Step: 10</p><p>State: 16</p><p>Action: drop-off</p><p>Reward: 19</p><p>Sum of Rewards: 10</p></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test after running the same initialization several times\n",
    "\n",
    "for i in range(1000):\n",
    "    taxi.initialize((3, 1), 2, 0)\n",
    "    taxi.transporter() \n",
    "\n",
    "taxi.initialize((3, 1), 2, 0)\n",
    "hist = taxi.transporter(plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((3, 1, 2, 0), 'backwards', -1, False),\n",
       " ((2, 1, 2, 0), 'Left', -1, False),\n",
       " ((2, 0, 2, 0), 'moving forward', -1, False),\n",
       " ((3, 0, 2, 0), 'moving forward', -1, False),\n",
       " ((4, 0, 2, 0), 'Pick-up', -1, False),\n",
       " ((4, 0, 4, 0), 'backwards', -1, False),\n",
       " ((3, 0, 4, 0), 'backwards', -1, False),\n",
       " ((2, 0, 4, 0), 'backwards', -1, False),\n",
       " ((1, 0, 4, 0), 'backwards', -1, False),\n",
       " ((0, 0, 4, 0), 'drop-off', 19, True)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to print the history of the steps\n",
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "                div.cont {display:inline-block; margin:5px; vertical-align: top;}\n",
       "                table#t, table#t td, table#t tr {border: 1px dotted black; \n",
       "                                                background: white; padding: 1px;}\n",
       "                \n",
       "                table#t td {width: 1cm; height:1cm; text-align: center;}\n",
       "                table#t tr td.l {border-left: 2px solid red ;}\n",
       "                table#t tr td.r {border-right: 2px solid red ;}\n",
       "                table#t tr td.b {border-bottom: 2px solid red ;}\n",
       "                table#t tr td.t {border-top: 2px solid red ;}\n",
       "                table#t tr td.stop {background: yellow;}\n",
       "                </style>\n",
       "                <div class=\"cont\">\n",
       "                <table id=\"t\">\n",
       "                <tr><td class=\"l t stop\">:</td><td class=\"r t \">:</td><td class=\"l t \">:</td><td class=\"t \">:</td><td class=\"r t stop\">üè≤üëΩüöñ</td></tr><tr><td class=\"l \">:</td><td >:</td><td >:</td><td >:</td><td class=\"r \">:</td></tr><tr><td class=\"l \">:</td><td >:</td><td >:</td><td >:</td><td class=\"r \">:</td></tr><tr><td class=\"l r \">:</td><td class=\"l \">:</td><td class=\"r \">:</td><td class=\"l \">:</td><td class=\"r \">:</td></tr><tr><td class=\"l r b stop\">:</td><td class=\"l b \">:</td><td class=\"r b \">:</td><td class=\"l b stop\">:</td><td class=\"r b \">:</td></tr></table></div><div class=\"cont\"><p>Step: 15</p><p>State: 97</p><p>Action: drop-off</p><p>Reward: 19</p><p>Sum of Rewards: 5</p></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the learning with random initializations\n",
    "\n",
    "stops2 = [(0,0), (0,4), (4,0), (4,3)]\n",
    "barriers2 = {\n",
    "    (0, 1): set([1]), # right-hand barrier\n",
    "    (0, 2): set([0]), # left-hand barrier\n",
    "    (3, 0): set([1]), # right-hand barrier\n",
    "    (4, 0): set([1]), # right-hand barrier\n",
    "    (3, 1): set([0]), # left-hand barrier\n",
    "    (4, 1): set([0]), # left-hand barrier\n",
    "    (3, 2): set([1]), # right-hand barrier\n",
    "    (4, 2): set([1]), # right-hand barrier\n",
    "    (3, 3): set([0]), # left-hand barrier\n",
    "    (4, 3): set([0]), # left-hand barrier\n",
    "}\n",
    "\n",
    "taxi2 = TaxiEnv(5, 5, stops2, barriers2)\n",
    "taxi2.add_agent(0.1, 0.1)\n",
    "\n",
    "def random_exec(taxi_env, plot=False):\n",
    "    pos = np.random.randint(5), np.random.randint(5)\n",
    "    psg, dst = np.random.randint(len(stops2)), np.random.randint(len(stops2))\n",
    "    taxi_env.initialize(pos, psg, dst)\n",
    "    return taxi_env.transporter(plot=plot) \n",
    "\n",
    "for i in range(10000):\n",
    "    random_exec(taxi2, plot=False)\n",
    "    \n",
    "print('End')\n",
    "\n",
    "hist = random_exec(taxi2, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Application and analysis\n",
    "\n",
    "Here are some tools for testing reinforcement learning:\n",
    "\n",
    "- OpenAI Baselines: https://github.com/openai/baselines\n",
    "- Intel Coach: https://github.com/IntelLabs/coach\n",
    "- Stable Baselines: https://github.com/DLR-RM/stable-baselines3\n",
    "- TF-Agents: https://github.com/tensorflow/agents\n",
    "- Keras-RL: https://github.com/keras-rl/keras-rl\n",
    "- Tensorforce: https://github.com/tensorforce/tensorforce\n",
    "- Chainer RL: https://github.com/chainer/chainerrl\n",
    "- Mushroom RL: https://github.com/MushroomRL/mushroom-rl\n",
    "- Acme: https://github.com/deepmind/acme\n",
    "- Dopamine: https://github.com/google/dopamine\n",
    "- RAY: https://github.com/ray-project/ray\n",
    "\n",
    "Environments :\n",
    "\n",
    "- Gym: https://gym.openai.com/\n",
    "- iGibson: http://svl.stanford.edu/igibson/\n",
    "\n",
    "We will use \"MushroomRL\" since the tool implements the classical methods.\n",
    "Also, we will use [\"Gym\"](https://gym.openai.com/) to generate the environments. \n",
    "The Taxi environment will be used since it doesn't consume a lot of resources (memory and computation).\n",
    "You can consult \"Gym\" for more complex environments like games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting setuptools==65.5.0\n",
      "  Using cached setuptools-65.5.0-py3-none-any.whl (1.2 MB)\n",
      "Installing collected packages: setuptools\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 67.8.0\n",
      "    Uninstalling setuptools-67.8.0:\n",
      "      Successfully uninstalled setuptools-67.8.0\n",
      "  Rolling back uninstall of setuptools\n",
      "  Moving to c:\\users\\minh-hoang\\appdata\\roaming\\python\\python39\\site-packages\\_distutils_hack\\\n",
      "   from C:\\Users\\Minh-Hoang\\AppData\\Roaming\\Python\\Python39\\site-packages\\~distutils_hack\n",
      "  Moving to c:\\users\\minh-hoang\\appdata\\roaming\\python\\python39\\site-packages\\distutils-precedence.pth\n",
      "   from C:\\Users\\Minh-Hoang\\AppData\\Local\\Temp\\pip-uninstall-hhvh58nh\\distutils-precedence.pth\n",
      "  Moving to c:\\users\\minh-hoang\\appdata\\roaming\\python\\python39\\site-packages\\pkg_resources\\\n",
      "   from C:\\Users\\Minh-Hoang\\AppData\\Roaming\\Python\\Python39\\site-packages\\~kg_resources\n",
      "  Moving to c:\\users\\minh-hoang\\appdata\\roaming\\python\\python39\\site-packages\\setuptools-67.8.0.dist-info\\\n",
      "   from C:\\Users\\Minh-Hoang\\AppData\\Roaming\\Python\\Python39\\site-packages\\~etuptools-67.8.0.dist-info\n",
      "  Moving to c:\\users\\minh-hoang\\appdata\\roaming\\python\\python39\\site-packages\\setuptools\\\n",
      "   from C:\\Users\\Minh-Hoang\\AppData\\Roaming\\Python\\Python39\\site-packages\\~etuptools\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -nyio (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -nyio (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'c:\\\\ProgramData\\\\Miniconda3\\\\Lib\\\\site-packages\\\\distutils-precedence.pth'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym==0.20.0\n",
      "  Using cached gym-0.20.0.tar.gz (1.6 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -nyio (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\programdata\\miniconda3\\lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  √ó python setup.py egg_info did not run successfully.\n",
      "  ‚îÇ exit code: 1\n",
      "  ‚ï∞‚îÄ> [1 lines of output]\n",
      "      error in gym setup command: 'extras_require' must be a dictionary whose values are strings or lists of strings containing valid project/version requirement specifiers.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "√ó Encountered error while generating package metadata.\n",
      "‚ï∞‚îÄ> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: Invalid requirement: 'mushroom_rl=1.7.0'\n",
      "Hint: = is not a valid operator. Did you mean == ?\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# package installation\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install setuptools==65.5.0\n",
    "!{sys.executable} -m pip install gym==0.20.0\n",
    "!{sys.executable} -m pip install mushroom_rl=1.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1. Exploration vs. Exploitation\n",
    "\n",
    "Here, we want to test the effect of exploration/exploitation trade-off. \n",
    "To do this, we will test with different values of epsilon :\n",
    "\n",
    "- 0.0 : exploitation (100%).\n",
    "- 0.5 : exploitation (50%) and exploration (50%).\n",
    "- 0.9 : exploration (90%) and exploitation (10%).\n",
    "\n",
    "The default maximum number of steps is 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'episodes_length' from 'mushroom_rl.utils.dataset' (c:\\ProgramData\\Miniconda3\\lib\\site-packages\\mushroom_rl\\utils\\dataset.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32me:\\Documents\\Dev\\Python\\Defi IA\\up_machine_learning\\session-6-reinforcement-learning\\LAB06_Minh-Hoang_HUYNH.ipynb Cell 43\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Documents/Dev/Python/Defi%20IA/up_machine_learning/session-6-reinforcement-learning/LAB06_Minh-Hoang_HUYNH.ipynb#X60sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmushroom_rl\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m \u001b[39mimport\u001b[39;00m CollectDataset\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/Documents/Dev/Python/Defi%20IA/up_machine_learning/session-6-reinforcement-learning/LAB06_Minh-Hoang_HUYNH.ipynb#X60sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmushroom_rl\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallbacks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallback\u001b[39;00m \u001b[39mimport\u001b[39;00m Callback\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/Documents/Dev/Python/Defi%20IA/up_machine_learning/session-6-reinforcement-learning/LAB06_Minh-Hoang_HUYNH.ipynb#X60sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmushroom_rl\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset\u001b[39;00m \u001b[39mimport\u001b[39;00m parse_dataset, episodes_length\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Documents/Dev/Python/Defi%20IA/up_machine_learning/session-6-reinforcement-learning/LAB06_Minh-Hoang_HUYNH.ipynb#X60sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# number of executions\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Documents/Dev/Python/Defi%20IA/up_machine_learning/session-6-reinforcement-learning/LAB06_Minh-Hoang_HUYNH.ipynb#X60sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m NBR_EPISODES \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m \n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'episodes_length' from 'mushroom_rl.utils.dataset' (c:\\ProgramData\\Miniconda3\\lib\\site-packages\\mushroom_rl\\utils\\dataset.py)"
     ]
    }
   ],
   "source": [
    "from mushroom_rl.core import Environment\n",
    "from mushroom_rl.policy import EpsGreedy\n",
    "from mushroom_rl.algorithms.value import QLearning\n",
    "from mushroom_rl.utils.dataset import compute_J\n",
    "from mushroom_rl.utils.parameters import Parameter\n",
    "from mushroom_rl.core import Core\n",
    "from mushroom_rl.utils.callbacks import CollectDataset\n",
    "from mushroom_rl.utils.callbacks.callback import Callback\n",
    "from mushroom_rl.utils.dataset import parse_dataset, episodes_length\n",
    "\n",
    "# number of executions\n",
    "NBR_EPISODES = 1000 \n",
    "\n",
    "env = Environment.make('Gym', 'Taxi-v3')\n",
    "\n",
    "epsilons = [.0, .5, .9]\n",
    "\n",
    "tests = []\n",
    "\n",
    "for eps in epsilons:\n",
    "    epsilon = Parameter(value=eps)\n",
    "    pi = EpsGreedy(epsilon=epsilon)\n",
    "    agent = QLearning(env.info, pi, learning_rate=Parameter(value=.3))\n",
    "    \n",
    "    collect_dataset = CollectDataset()\n",
    "    callbacks = [collect_dataset]\n",
    "    \n",
    "    core = Core(agent, env, callbacks_fit=callbacks)\n",
    "    core.learn(n_episodes=NBR_EPISODES, n_steps_per_fit=1)\n",
    "    \n",
    "    res = {}\n",
    "    res['nbr_steps'] = episodes_length(collect_dataset.get())\n",
    "    tests.append(res)\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "for i, test in enumerate(tests):\n",
    "    plt.plot(test['nbr_steps'], label='epsilon=' + str(epsilons[i]))\n",
    "plt.xlabel('Episodes') \n",
    "plt.ylabel('Number of steps') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\\[Q1\\]: Analyze the results:**\n",
    "\n",
    "1. Why can't the algorithm with more exploration minimize the number of steps after several sessions (episodes) ?\n",
    "2. Why there are episodes that have a minimal number of steps especially in the last episodes (always in the algorithm with more exploration) ?\n",
    "3. Why does the one with only exploitation take less steps in each episode ?\n",
    "4. In this case, what is the purpose of the exploration ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[A1:Begin]\n",
    "\n",
    "**Answer 01**\n",
    "\n",
    "1. ...\n",
    "2. ...\n",
    "3. ...\n",
    "4. ...\n",
    "\n",
    "[A1:End]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2. Learning rate\n",
    "\n",
    "In this last part, we will investigate the effect of the learning rate on the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of executions\n",
    "NBR_EPISODES = 1000\n",
    "\n",
    "env = Environment.make('Gym', 'Taxi-v3')\n",
    "\n",
    "lrs = [.1, .2, .3]\n",
    "\n",
    "tests = []\n",
    "\n",
    "for lr in lrs:\n",
    "    epsilon = Parameter(value=0.1)\n",
    "    pi = EpsGreedy(epsilon=epsilon)\n",
    "    agent = QLearning(env.info, pi, learning_rate=Parameter(value=lr))\n",
    "    \n",
    "    collect_dataset = CollectDataset()\n",
    "    callbacks = [collect_dataset]\n",
    "    \n",
    "    core = Core(agent, env, callbacks_fit=callbacks)\n",
    "    core.learn(n_episodes=NBR_EPISODES, n_steps_per_fit=1)\n",
    "    \n",
    "    res = {}\n",
    "    res['nbr_steps'] = episodes_length(collect_dataset.get())\n",
    "    tests.append(res)\n",
    "    \n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "for i, test in enumerate(tests):\n",
    "    plt.plot(test['nbr_steps'], label='Learning Rates=' + str(lrs[i]))\n",
    "plt.xlabel('Episodes') \n",
    "plt.ylabel('Number of steps') \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\\[Q2\\]: Analyze the results:**\n",
    "\n",
    "- What is the effect of $\\alpha$ on the number of steps after each $n$ episodes ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[A2:Begin]\n",
    "\n",
    "**Answer 02**\n",
    "\n",
    "1. ...\n",
    "2. ...\n",
    "3. ...\n",
    "4. ...\n",
    "\n",
    "[A2:End]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\\[Q3\\]: Analyze the results:**\n",
    "\n",
    "Looking at this diagram and the one before, we can notice that :\n",
    "- The evolution with $\\epsilon=0$ is almost like the one with $\\alpha=0.3$ \n",
    "- and the evolution with $\\epsilon=0.5$ is almost like the one with $\\alpha=0.1$. \n",
    "\n",
    "In this case, can we say that there is a direct relation between the two parameters (in other words, can we replace one othe them as a function of the other) ? if yes, can you explain that ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[A3:Begin]\n",
    "\n",
    "**Answer 03**\n",
    "\n",
    " - ...\n",
    " - ...\n",
    "\n",
    "[A2:End]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
